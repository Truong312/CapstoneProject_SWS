<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Whisper Voice-to-Text Demo</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 700px; margin: 40px auto; text-align: center; }
        h1 { color: #333; }
        button { padding: 10px 20px; margin: 10px; font-size: 16px; cursor: pointer; }
        textarea { width: 100%; height: 180px; margin-top: 20px; }
        #status { margin-top: 10px; color: #666; }
    </style>
</head>
<body>
<h1>ğŸ™ï¸ Whisper Voice-to-Text Demo</h1>

<button id="startBtn">Báº¯t Ä‘áº§u ghi Ã¢m</button>
<button id="stopBtn" disabled>Dá»«ng ghi Ã¢m</button>
<div id="status">Nháº¥n "Báº¯t Ä‘áº§u ghi Ã¢m" Ä‘á»ƒ thá»­.</div>

<audio id="audioPlayback" controls style="margin-top:20px; display:none;"></audio>

<h3>Káº¿t quáº£:</h3>
<textarea id="result" placeholder="Káº¿t quáº£ text sáº½ hiá»ƒn thá»‹ á»Ÿ Ä‘Ã¢y..."></textarea>

<script>
    let mediaRecorder;
    let audioChunks = [];

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const resultBox = document.getElementById("result");
    const status = document.getElementById("status");
    const audioPlayback = document.getElementById("audioPlayback");

    // ğŸ™ï¸ Báº¯t Ä‘áº§u ghi Ã¢m
    startBtn.addEventListener("click", async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];

            mediaRecorder.ondataavailable = event => {
                if (event.data.size > 0) audioChunks.push(event.data);
            };

            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlayback.src = audioUrl;
                audioPlayback.style.display = "block";
                await uploadAudio(audioBlob);
            };

            mediaRecorder.start();
            startBtn.disabled = true;
            stopBtn.disabled = false;
            status.textContent = "ğŸ§ Äang ghi Ã¢m... nÃ³i Ä‘i!";
        } catch (err) {
            console.error(err);
            alert("KhÃ´ng thá»ƒ truy cáº­p micro: " + err.message);
        }
    });

    // â¹ Dá»«ng ghi Ã¢m
    stopBtn.addEventListener("click", () => {
        mediaRecorder.stop();
        startBtn.disabled = false;
        stopBtn.disabled = true;
        status.textContent = "â³ Äang xá»­ lÃ½ Ã¢m thanh...";
    });

    // ğŸš€ Gá»­i audio lÃªn server Whisper
    async function uploadAudio(blob) {
        resultBox.value = "Äang táº£i lÃªn vÃ  xá»­ lÃ½...";
        const formData = new FormData();
        formData.append("file", blob, "recording.webm");
        formData.append("model", "whisper-1"); // náº¿u API yÃªu cáº§u
        formData.append("language", "vi"); // hoáº·c "en"

        try {
            const response = await fetch("http://localhost:10300/inference", {
                method: "POST",
                body: formData
            });

            if (!response.ok) throw new Error("API lá»—i: " + response.statusText);
            const data = await response.json();
            resultBox.value = data.text || JSON.stringify(data, null, 2);
            status.textContent = "âœ… HoÃ n táº¥t!";
        } catch (err) {
            console.error(err);
            resultBox.value = "âŒ Lá»—i: " + err.message;
            status.textContent = "âŒ Gá»­i tháº¥t báº¡i.";
        }
    }
</script>
</body>
</html>
